{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbEl6ASeLjCv"
      },
      "source": [
        "Surya Sekhar Dutta\n",
        "Chatbot for code clause\n",
        "task-2 Data Science Intern Code Clause"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBtYnyp5LguW"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot\n",
        "import random\n",
        "import nltk #natural language proccessing toolkit\n",
        "from nltk.stem import WordNetLemmatizer#A lemmatizer is a tool used\n",
        "#in natural language processing to reduce words to their base or root\n",
        "#form, known as the lemma. The process of reducing words to their\n",
        "#lemma is called lemmatization. For example, lemmatization\n",
        "#would convert words like \"running\" and \"ran\" to their lemma \"run.\"\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense,Activation,Dropout\n",
        "from keras.optimizers import SGD\n",
        "#In the code snippet from keras.optimizers import SGD, the statement is importing the\n",
        "#Stochastic Gradient Descent (SGD) optimizer from the Keras library.\n",
        "#In the context of deep learning and neural networks, an\n",
        "#optimizer is an algorithm used to update the weights and biases of\n",
        "#the neural network during the training process. The objective of\n",
        "#training a neural network is to find the optimal set of weights that\n",
        "#minimize the error (loss) between the predicted outputs and the\n",
        "#actual targets. The optimizer helps to adjust the weights and biases\n",
        "#in a way that gradually reduces the error and improves the model's performance.\n",
        "#Stochastic Gradient Descent is one of the most fundamental\n",
        "#and widely used optimization algorithms in deep learning.\n",
        "#It is a variant of the traditional Gradient Descent algorithm\n",
        "#but processes a single training sample at a time instead of\n",
        "#the entire dataset. This characteristic makes it computationally\n",
        "#more efficient, especially for large datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzvMykqEK1WC",
        "outputId": "13eed21b-db97-4615-b1ee-7ff40a56dde1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#download nlkt resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcwGjCeQLYuM"
      },
      "outputs": [],
      "source": [
        "#load json file\n",
        "intents = json.loads('''\n",
        "{\n",
        "  \"intents\": [\n",
        "    {\n",
        "      \"tag\": \"greeting\",\n",
        "      \"patterns\": [\"Hi there\", \"How are you\", \"Is anyone there?\", \"Hey\", \"Hola\", \"Hello\", \"Good day\"],\n",
        "      \"responses\": [\"Hello, thanks for asking\", \"Good to see you again\", \"Hi there, how can I help?\"],\n",
        "      \"context\": [\"\"]\n",
        "    },\n",
        "     {\n",
        "      \"tag\": \"goodbye\",\n",
        "      \"patterns\": [\"Bye\", \"See you later\", \"Goodbye\", \"Nice chatting to you, bye\", \"Till next time\"],\n",
        "      \"responses\": [\"See you!\", \"Have a nice day\", \"Bye! Come back again soon.\"],\n",
        "      \"context\": [\"\"]\n",
        "    },\n",
        "     {\n",
        "      \"tag\": \"thanks\",\n",
        "      \"patterns\": [\"Thanks\", \"Thank you\", \"That's helpful\", \"Awesome, thanks\", \"Thanks for helping me\"],\n",
        "      \"responses\": [\"Happy to help!\", \"Any time!\", \"My pleasure\"],\n",
        "      \"context\": [\"\"]\n",
        "    },\n",
        "     {\n",
        "      \"tag\": \"noanswer\",\n",
        "      \"patterns\": [],\n",
        "      \"responses\": [\"Sorry, can't understand you\", \"Please give me more info\", \"Not sure I understand\"],\n",
        "      \"context\": [\"\"]\n",
        "    },\n",
        "        {\n",
        "      \"tag\": \"codeclause\",\n",
        "      \"patterns\": [\"About codeclause\", \"What is codeclause?\", \"Services offered\", \"What is codeclause internship?\", \"codeclause\"],\n",
        "      \"responses\": [\"We offer reliable, efficient delivery with high-caliber engineers...\", \"At CodeClause, we endeavor to provide a formidable platform...\"],\n",
        "      \"context\": [\"\"]\n",
        "    },\n",
        "    {\n",
        "      \"tag\": \"options\",\n",
        "      \"patterns\": [\"How you could help me?\", \"What you can do?\", \"What help you provide?\", \"How you can be helpful?\", \"What support is offered\"],\n",
        "      \"responses\": [\"I can give you info about the services and internships provided by codeclause.com\", \"We can provide you internships in various domains...\", \"Offering internships and services in...\"],\n",
        "      \"context\": [\"\"]\n",
        "    },\n",
        "    {\n",
        "      \"tag\": \"Internship\",\n",
        "      \"patterns\": [\"How to apply for the internship?\", \"Where to apply for internship?\", \"Internships offered\"],\n",
        "      \"responses\": [\"Please visit the website at https://codeclause.com/careers.html to apply for internships...\"],\n",
        "      \"context\": [\"\"]\n",
        "    },\n",
        "    {\n",
        "      \"tag\": \"offer_letter\",\n",
        "      \"patterns\": [\"Applied for internship\", \"I have applied for internship\", \"When will I receive my offer letter?\", \"offer letter not received\"],\n",
        "      \"responses\": [\"You will get your offer letter on mail around starting week of the month for which you've applied for the internship\"],\n",
        "      \"context\": [\"\"]\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "''')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMwpwfNhLYxB"
      },
      "outputs": [],
      "source": [
        "#extract worfs from intents\n",
        "words=[]\n",
        "missing_words=[\"!\",\"?\"]\n",
        "classes=[]\n",
        "documents=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7kyJHDrLYz_",
        "outputId": "32cf0ecb-f379-493d-cab6-d3dcb5fe3ba4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Internship',\n",
              " 'codeclause',\n",
              " 'goodbye',\n",
              " 'greeting',\n",
              " 'offer_letter',\n",
              " 'options',\n",
              " 'thanks']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for intent in intents['intents']:\n",
        "  for pattern in intent['patterns']:\n",
        "    words.extend(nltk.word_tokenize(pattern))\n",
        "    documents.append((nltk.word_tokenize(pattern),intent['tag']))\n",
        "    if intent['tag'] not in classes:\n",
        "      classes.append(intent['tag'])\n",
        "# Lemmatize words and remove duplicates\n",
        "words = list(set([WordNetLemmatizer().lemmatize(word.lower()) for word in words if word not in missing_words]))\n",
        "classes = sorted(classes)\n",
        "\n",
        "\"\"\"\n",
        "This line of code performs the following steps:\n",
        "It iterates through each word in the words list.\n",
        "For each word, it applies the following transformations:\n",
        "Converts the word to lowercase using word.lower().\n",
        "This is done to ensure that the words are case-insensitive, so \"Hello\" and \"hello\" are treated\n",
        "as the same word.\n",
        "Lemmatizes the word using WordNetLemmatizer().lemmatize(word.lower()).\n",
        "The lemmatization process reduces words to their base or root form (lemma). For example,\n",
        "lemmatizing \"running\" will result in \"run\".\n",
        "Checks if the lemmatized word is not present in the ignore_words list.\n",
        "The ignore_words list likely contains common words or stop words that should be excluded from\n",
        "the final set of words used for analysis.\n",
        "The resulting list of lemmatized words (after applying the lowercase transformation\n",
        "and lemmatization) is converted to a set to remove any duplicate words.\n",
        "Finally, the set of lemmatized words is converted back to a list and stored in the words variable.\n",
        "This ensures that words contains a list of unique, lemmatized words, excluding any words present\n",
        "in the ignore_words list.\n",
        "classes = sorted(classes)\n",
        "This line simply sorts the classes list in ascending order.\n",
        "The classes list contains unique intent tags (as strings) obtained from the data.\n",
        "Sorting the list ensures that the classes are in a consistent order, which can be helpful during\n",
        "training and prediction.\n",
        "After executing these lines of code, the words list will contain a unique set of lemmatized\n",
        "words, and the classes list will contain the sorted unique intent tags. These preprocessed lists\n",
        "can be used for further data processing and model training in a natural language processing\n",
        "or chatbot application.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-l55ClULY25",
        "outputId": "76486542-e28c-42a1-ee5a-ecb2297a65be"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 0, 0, 0, 0, 0, 0]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#create a train data\n",
        "train=[]\n",
        "output_empty=len(classes)*[0]\n",
        "output_empty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIw0xZOzLY5h",
        "outputId": "dd22de37-73f6-4807-f689-c5fd20a6226c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hi', 'there']\n",
            "['How', 'are', 'you']\n",
            "['Is', 'anyone', 'there', '?']\n",
            "['Hey']\n",
            "['Hola']\n",
            "['Hello']\n",
            "['Good', 'day']\n",
            "['Bye']\n",
            "['See', 'you', 'later']\n",
            "['Goodbye']\n",
            "['Nice', 'chatting', 'to', 'you', ',', 'bye']\n",
            "['Till', 'next', 'time']\n",
            "['Thanks']\n",
            "['Thank', 'you']\n",
            "['That', \"'s\", 'helpful']\n",
            "['Awesome', ',', 'thanks']\n",
            "['Thanks', 'for', 'helping', 'me']\n",
            "['About', 'codeclause']\n",
            "['What', 'is', 'codeclause', '?']\n",
            "['Services', 'offered']\n",
            "['What', 'is', 'codeclause', 'internship', '?']\n",
            "['codeclause']\n",
            "['How', 'you', 'could', 'help', 'me', '?']\n",
            "['What', 'you', 'can', 'do', '?']\n",
            "['What', 'help', 'you', 'provide', '?']\n",
            "['How', 'you', 'can', 'be', 'helpful', '?']\n",
            "['What', 'support', 'is', 'offered']\n",
            "['How', 'to', 'apply', 'for', 'the', 'internship', '?']\n",
            "['Where', 'to', 'apply', 'for', 'internship', '?']\n",
            "['Internships', 'offered']\n",
            "['Applied', 'for', 'internship']\n",
            "['I', 'have', 'applied', 'for', 'internship']\n",
            "['When', 'will', 'I', 'receive', 'my', 'offer', 'letter', '?']\n",
            "['offer', 'letter', 'not', 'received']\n"
          ]
        }
      ],
      "source": [
        "for doc in documents:\n",
        "  bag=[]\n",
        "  pattern_word=doc[0]\n",
        "  print(pattern_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1tuTkZmLY8X",
        "outputId": "d4e3e64f-5203-439f-c79f-9ca56bf8cd7d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'intents': [{'tag': 'greeting',\n",
              "   'patterns': ['Hi there',\n",
              "    'How are you',\n",
              "    'Is anyone there?',\n",
              "    'Hey',\n",
              "    'Hola',\n",
              "    'Hello',\n",
              "    'Good day'],\n",
              "   'responses': ['Hello, thanks for asking',\n",
              "    'Good to see you again',\n",
              "    'Hi there, how can I help?'],\n",
              "   'context': ['']},\n",
              "  {'tag': 'goodbye',\n",
              "   'patterns': ['Bye',\n",
              "    'See you later',\n",
              "    'Goodbye',\n",
              "    'Nice chatting to you, bye',\n",
              "    'Till next time'],\n",
              "   'responses': ['See you!', 'Have a nice day', 'Bye! Come back again soon.'],\n",
              "   'context': ['']},\n",
              "  {'tag': 'thanks',\n",
              "   'patterns': ['Thanks',\n",
              "    'Thank you',\n",
              "    \"That's helpful\",\n",
              "    'Awesome, thanks',\n",
              "    'Thanks for helping me'],\n",
              "   'responses': ['Happy to help!', 'Any time!', 'My pleasure'],\n",
              "   'context': ['']},\n",
              "  {'tag': 'noanswer',\n",
              "   'patterns': [],\n",
              "   'responses': [\"Sorry, can't understand you\",\n",
              "    'Please give me more info',\n",
              "    'Not sure I understand'],\n",
              "   'context': ['']},\n",
              "  {'tag': 'codeclause',\n",
              "   'patterns': ['About codeclause',\n",
              "    'What is codeclause?',\n",
              "    'Services offered',\n",
              "    'What is codeclause internship?',\n",
              "    'codeclause'],\n",
              "   'responses': ['We offer reliable, efficient delivery with high-caliber engineers...',\n",
              "    'At CodeClause, we endeavor to provide a formidable platform...'],\n",
              "   'context': ['']},\n",
              "  {'tag': 'options',\n",
              "   'patterns': ['How you could help me?',\n",
              "    'What you can do?',\n",
              "    'What help you provide?',\n",
              "    'How you can be helpful?',\n",
              "    'What support is offered'],\n",
              "   'responses': ['I can give you info about the services and internships provided by codeclause.com',\n",
              "    'We can provide you internships in various domains...',\n",
              "    'Offering internships and services in...'],\n",
              "   'context': ['']},\n",
              "  {'tag': 'Internship',\n",
              "   'patterns': ['How to apply for the internship?',\n",
              "    'Where to apply for internship?',\n",
              "    'Internships offered'],\n",
              "   'responses': ['Please visit the website at https://codeclause.com/careers.html to apply for internships...'],\n",
              "   'context': ['']},\n",
              "  {'tag': 'offer_letter',\n",
              "   'patterns': ['Applied for internship',\n",
              "    'I have applied for internship',\n",
              "    'When will I receive my offer letter?',\n",
              "    'offer letter not received'],\n",
              "   'responses': [\"You will get your offer letter on mail around starting week of the month for which you've applied for the internship\"],\n",
              "   'context': ['']}]}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "intents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJktnUTeLY_P",
        "outputId": "10a462c2-b8ac-4f24-9476-29151076f81d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['offer', 'letter', 'not', 'received']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pattern_words = [WordNetLemmatizer().lemmatize(word.lower()) for word in pattern_word]\n",
        "pattern_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPZmpz3HLZCs",
        "outputId": "cbce8027-5779-452b-e6e9-66536ed0d151"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "for word in words:\n",
        "  bag.append(1) if word in pattern_words else bag.append(0)\n",
        "  output_row=list(output_empty)\n",
        "  output_row[classes.index(doc[1])] = 1\n",
        "  print(output_row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oBsbLI4_KGW",
        "outputId": "1f4b5a68-1c1f-4974-d368-7d422e455623"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[[0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   1,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   1,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   1,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   1,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0],\n",
              "  [0, 0, 0, 0, 1, 0, 0]]]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.append([bag,output_row])\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXwQTLAL_KJd",
        "outputId": "239efabb-a07e-4611-806c-ca479fe71900"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-12-cf35fa34b05d>:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  training=np.array(train)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[[0, 0, 0, 0, 1, 0, 0]]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "random.shuffle(train)\n",
        "training=np.array(train)\n",
        "train_x=list(training[:,0])\n",
        "train_y=list(training[:,1])\n",
        "train_x\n",
        "print(\"\\n\")\n",
        "train_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoIn1tjN_KMs",
        "outputId": "53cdfc34-a24a-4965-920d-fbacaa380c94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "1/1 [==============================] - 1s 711ms/step - loss: 2.3161 - accuracy: 0.0000e+00\n",
            "Epoch 2/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.4596 - accuracy: 1.0000\n",
            "Epoch 3/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.5584 - accuracy: 1.0000\n",
            "Epoch 4/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1474 - accuracy: 1.0000\n",
            "Epoch 5/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.8151 - accuracy: 1.0000\n",
            "Epoch 6/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.3079 - accuracy: 1.0000\n",
            "Epoch 7/250\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0020 - accuracy: 1.0000\n",
            "Epoch 8/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.9868 - accuracy: 1.0000\n",
            "Epoch 9/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3734 - accuracy: 1.0000\n",
            "Epoch 10/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5974 - accuracy: 1.0000\n",
            "Epoch 11/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0569 - accuracy: 1.0000\n",
            "Epoch 12/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1515 - accuracy: 1.0000\n",
            "Epoch 13/250\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1950 - accuracy: 1.0000\n",
            "Epoch 14/250\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0288 - accuracy: 1.0000\n",
            "Epoch 15/250\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 9.0927e-04 - accuracy: 1.0000\n",
            "Epoch 16/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0041 - accuracy: 1.0000\n",
            "Epoch 17/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 18/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0039 - accuracy: 1.0000\n",
            "Epoch 19/250\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 4.5611e-04 - accuracy: 1.0000\n",
            "Epoch 20/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 21/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 22/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.7881e-06 - accuracy: 1.0000\n",
            "Epoch 23/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.7483e-05 - accuracy: 1.0000\n",
            "Epoch 24/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 9.9062e-04 - accuracy: 1.0000\n",
            "Epoch 25/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.0252e-05 - accuracy: 1.0000\n",
            "Epoch 26/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.4571e-06 - accuracy: 1.0000\n",
            "Epoch 27/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.5616e-05 - accuracy: 1.0000\n",
            "Epoch 28/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 3.3480e-04 - accuracy: 1.0000\n",
            "Epoch 29/250\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 6.4967e-05 - accuracy: 1.0000\n",
            "Epoch 30/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.3842e-07 - accuracy: 1.0000\n",
            "Epoch 31/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.8967e-05 - accuracy: 1.0000\n",
            "Epoch 32/250\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 3.5763e-07 - accuracy: 1.0000\n",
            "Epoch 33/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.3842e-07 - accuracy: 1.0000\n",
            "Epoch 34/250\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 7.1525e-06 - accuracy: 1.0000\n",
            "Epoch 35/250\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0056 - accuracy: 1.0000\n",
            "Epoch 36/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 5.2452e-06 - accuracy: 1.0000\n",
            "Epoch 37/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.8137e-04 - accuracy: 1.0000\n",
            "Epoch 38/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0048 - accuracy: 1.0000\n",
            "Epoch 39/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.8239e-05 - accuracy: 1.0000\n",
            "Epoch 40/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.0168e-04 - accuracy: 1.0000\n",
            "Epoch 41/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.3842e-07 - accuracy: 1.0000\n",
            "Epoch 42/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.0981e-05 - accuracy: 1.0000\n",
            "Epoch 43/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.4709e-04 - accuracy: 1.0000\n",
            "Epoch 44/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 45/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 5.7931e-04 - accuracy: 1.0000\n",
            "Epoch 46/250\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 47/250\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0729e-06 - accuracy: 1.0000\n",
            "Epoch 48/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 49/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.6703e-05 - accuracy: 1.0000\n",
            "Epoch 50/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 51/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1921e-06 - accuracy: 1.0000\n",
            "Epoch 52/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1921e-07 - accuracy: 1.0000\n",
            "Epoch 53/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0093 - accuracy: 1.0000\n",
            "Epoch 54/250\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.3899e-04 - accuracy: 1.0000\n",
            "Epoch 55/250\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 56/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.1126e-05 - accuracy: 1.0000\n",
            "Epoch 57/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 7.0333e-06 - accuracy: 1.0000\n",
            "Epoch 58/250\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.3842e-07 - accuracy: 1.0000\n",
            "Epoch 59/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.3113e-06 - accuracy: 1.0000\n",
            "Epoch 60/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 5.8050e-04 - accuracy: 1.0000\n",
            "Epoch 61/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0085 - accuracy: 1.0000\n",
            "Epoch 62/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 63/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.9095e-04 - accuracy: 1.0000\n",
            "Epoch 64/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.5063e-04 - accuracy: 1.0000\n",
            "Epoch 65/250\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 66/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.0437e-05 - accuracy: 1.0000\n",
            "Epoch 67/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.8239e-05 - accuracy: 1.0000\n",
            "Epoch 68/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.6955e-06 - accuracy: 1.0000\n",
            "Epoch 69/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 70/250\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 71/250\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 6.7426e-04 - accuracy: 1.0000\n",
            "Epoch 72/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.5763e-06 - accuracy: 1.0000\n",
            "Epoch 73/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.3351e-05 - accuracy: 1.0000\n",
            "Epoch 74/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.7881e-06 - accuracy: 1.0000\n",
            "Epoch 75/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.6689e-05 - accuracy: 1.0000\n",
            "Epoch 76/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 7.2717e-06 - accuracy: 1.0000\n",
            "Epoch 77/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.3874e-04 - accuracy: 1.0000\n",
            "Epoch 78/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.5763e-07 - accuracy: 1.0000\n",
            "Epoch 79/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 8.8330e-05 - accuracy: 1.0000\n",
            "Epoch 80/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.4543e-05 - accuracy: 1.0000\n",
            "Epoch 81/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0299e-04 - accuracy: 1.0000\n",
            "Epoch 82/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 83/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3842e-07 - accuracy: 1.0000\n",
            "Epoch 84/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 85/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.5616e-05 - accuracy: 1.0000\n",
            "Epoch 86/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 87/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3842e-07 - accuracy: 1.0000\n",
            "Epoch 88/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 89/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 90/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 9.4767e-05 - accuracy: 1.0000\n",
            "Epoch 91/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 92/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 8.9999e-05 - accuracy: 1.0000\n",
            "Epoch 93/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.1457e-04 - accuracy: 1.0000\n",
            "Epoch 94/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0359 - accuracy: 1.0000\n",
            "Epoch 95/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.4212e-05 - accuracy: 1.0000\n",
            "Epoch 96/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 97/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.0266e-06 - accuracy: 1.0000\n",
            "Epoch 98/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.0809e-04 - accuracy: 1.0000\n",
            "Epoch 99/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.4676e-05 - accuracy: 1.0000\n",
            "Epoch 100/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 101/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 102/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.3126e-05 - accuracy: 1.0000\n",
            "Epoch 103/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 8.2254e-06 - accuracy: 1.0000\n",
            "Epoch 104/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 105/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 106/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.7418e-06 - accuracy: 1.0000\n",
            "Epoch 107/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.5034e-06 - accuracy: 1.0000\n",
            "Epoch 108/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.8082e-04 - accuracy: 1.0000\n",
            "Epoch 109/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 110/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.3379e-06 - accuracy: 1.0000\n",
            "Epoch 111/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 112/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.7684e-06 - accuracy: 1.0000\n",
            "Epoch 113/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.3644e-06 - accuracy: 1.0000\n",
            "Epoch 114/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.1921e-07 - accuracy: 1.0000\n",
            "Epoch 115/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.8835e-05 - accuracy: 1.0000\n",
            "Epoch 116/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0729e-06 - accuracy: 1.0000\n",
            "Epoch 117/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.1921e-07 - accuracy: 1.0000\n",
            "Epoch 118/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.1921e-07 - accuracy: 1.0000\n",
            "Epoch 119/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.7418e-06 - accuracy: 1.0000\n",
            "Epoch 120/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.2528e-04 - accuracy: 1.0000\n",
            "Epoch 121/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.0266e-06 - accuracy: 1.0000\n",
            "Epoch 122/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1618 - accuracy: 1.0000\n",
            "Epoch 123/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 124/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.5299e-06 - accuracy: 1.0000\n",
            "Epoch 125/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.4809e-05 - accuracy: 1.0000\n",
            "Epoch 126/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 127/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.4186e-05 - accuracy: 1.0000\n",
            "Epoch 128/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.8678e-06 - accuracy: 1.0000\n",
            "Epoch 129/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 130/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.8027e-05 - accuracy: 1.0000\n",
            "Epoch 131/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.1526e-07 - accuracy: 1.0000\n",
            "Epoch 132/250\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.5153e-05 - accuracy: 1.0000\n",
            "Epoch 133/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.5034e-06 - accuracy: 1.0000\n",
            "Epoch 134/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.1921e-07 - accuracy: 1.0000\n",
            "Epoch 135/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.2557e-05 - accuracy: 1.0000\n",
            "Epoch 136/250\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0729e-05 - accuracy: 1.0000\n",
            "Epoch 137/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.2981e-04 - accuracy: 1.0000\n",
            "Epoch 138/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.8185e-05 - accuracy: 1.0000\n",
            "Epoch 139/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 140/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.1113e-05 - accuracy: 1.0000\n",
            "Epoch 141/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.6032e-04 - accuracy: 1.0000\n",
            "Epoch 142/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 143/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0055 - accuracy: 1.0000\n",
            "Epoch 144/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.6689e-06 - accuracy: 1.0000\n",
            "Epoch 145/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.8142e-04 - accuracy: 1.0000\n",
            "Epoch 146/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.0398e-05 - accuracy: 1.0000\n",
            "Epoch 147/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.7643e-05 - accuracy: 1.0000\n",
            "Epoch 148/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.4543e-05 - accuracy: 1.0000\n",
            "Epoch 149/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3842e-07 - accuracy: 1.0000\n",
            "Epoch 150/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3842e-07 - accuracy: 1.0000\n",
            "Epoch 151/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 152/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.2715e-05 - accuracy: 1.0000\n",
            "Epoch 153/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.5763e-06 - accuracy: 1.0000\n",
            "Epoch 154/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 8.4635e-05 - accuracy: 1.0000\n",
            "Epoch 155/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 156/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.7949e-06 - accuracy: 1.0000\n",
            "Epoch 157/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 158/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.3842e-07 - accuracy: 1.0000\n",
            "Epoch 159/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.9739e-04 - accuracy: 1.0000\n",
            "Epoch 160/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 161/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 162/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0070 - accuracy: 1.0000\n",
            "Epoch 163/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.1526e-07 - accuracy: 1.0000\n",
            "Epoch 164/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 165/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 166/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 167/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.3842e-06 - accuracy: 1.0000\n",
            "Epoch 168/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 5.1260e-06 - accuracy: 1.0000\n",
            "Epoch 169/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 170/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0569 - accuracy: 1.0000\n",
            "Epoch 171/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0478e-04 - accuracy: 1.0000\n",
            "Epoch 172/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.5763e-07 - accuracy: 1.0000\n",
            "Epoch 173/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.9073e-06 - accuracy: 1.0000\n",
            "Epoch 174/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 8.0732e-04 - accuracy: 1.0000\n",
            "Epoch 175/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0133e-05 - accuracy: 1.0000\n",
            "Epoch 176/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 5.5788e-05 - accuracy: 1.0000\n",
            "Epoch 177/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 178/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 179/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1479e-04 - accuracy: 1.0000\n",
            "Epoch 180/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 9.3575e-05 - accuracy: 1.0000\n",
            "Epoch 181/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 3.5285e-05 - accuracy: 1.0000\n",
            "Epoch 182/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 5.7934e-05 - accuracy: 1.0000\n",
            "Epoch 183/250\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 3.5762e-05 - accuracy: 1.0000\n",
            "Epoch 184/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 185/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.9789e-05 - accuracy: 1.0000\n",
            "Epoch 186/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.2650e-06 - accuracy: 1.0000\n",
            "Epoch 187/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 188/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.5448e-04 - accuracy: 1.0000\n",
            "Epoch 189/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.3828e-05 - accuracy: 1.0000\n",
            "Epoch 190/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 191/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.9073e-06 - accuracy: 1.0000\n",
            "Epoch 192/250\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 193/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.6808e-05 - accuracy: 1.0000\n",
            "Epoch 194/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.6689e-06 - accuracy: 1.0000\n",
            "Epoch 195/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.8265e-05 - accuracy: 1.0000\n",
            "Epoch 196/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0171 - accuracy: 1.0000\n",
            "Epoch 197/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.1921e-07 - accuracy: 1.0000\n",
            "Epoch 198/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 199/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 200/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.8147e-06 - accuracy: 1.0000\n",
            "Epoch 201/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 202/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.3842e-07 - accuracy: 1.0000\n",
            "Epoch 203/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.7684e-07 - accuracy: 1.0000\n",
            "Epoch 204/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 205/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.5329e-04 - accuracy: 1.0000\n",
            "Epoch 206/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.3113e-06 - accuracy: 1.0000\n",
            "Epoch 207/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 4.5299e-06 - accuracy: 1.0000\n",
            "Epoch 208/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 209/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 8.5830e-06 - accuracy: 1.0000\n",
            "Epoch 210/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.4107e-06 - accuracy: 1.0000\n",
            "Epoch 211/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.3842e-07 - accuracy: 1.0000\n",
            "Epoch 212/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 9.8943e-06 - accuracy: 1.0000\n",
            "Epoch 213/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.6689e-06 - accuracy: 1.0000\n",
            "Epoch 214/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.8876e-06 - accuracy: 1.0000\n",
            "Epoch 215/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.7418e-05 - accuracy: 1.0000\n",
            "Epoch 216/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 217/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 218/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 219/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 220/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 221/250\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.4447e-04 - accuracy: 1.0000\n",
            "Epoch 222/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.1921e-07 - accuracy: 1.0000\n",
            "Epoch 223/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.5305e-04 - accuracy: 1.0000\n",
            "Epoch 224/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 225/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 226/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.5034e-06 - accuracy: 1.0000\n",
            "Epoch 227/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 228/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.5497e-06 - accuracy: 1.0000\n",
            "Epoch 229/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.0266e-06 - accuracy: 1.0000\n",
            "Epoch 230/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 231/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1921e-07 - accuracy: 1.0000\n",
            "Epoch 232/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3842e-07 - accuracy: 1.0000\n",
            "Epoch 233/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.6808e-05 - accuracy: 1.0000\n",
            "Epoch 234/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.3788e-05 - accuracy: 1.0000\n",
            "Epoch 235/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 236/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 237/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.3842e-07 - accuracy: 1.0000\n",
            "Epoch 238/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 239/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.8279e-05 - accuracy: 1.0000\n",
            "Epoch 240/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.7895e-04 - accuracy: 1.0000\n",
            "Epoch 241/250\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 242/250\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.0240e-04 - accuracy: 1.0000\n",
            "Epoch 243/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1921e-07 - accuracy: 1.0000\n",
            "Epoch 244/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.2099e-04 - accuracy: 1.0000\n",
            "Epoch 245/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0026 - accuracy: 1.0000\n",
            "Epoch 246/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 247/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.0266e-06 - accuracy: 1.0000\n",
            "Epoch 248/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.3728e-04 - accuracy: 1.0000\n",
            "Epoch 249/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 250/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.7684e-07 - accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "# Build and train the model\n",
        "sgd = SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "model.fit(np.array(train_x), np.array(train_y), epochs=250, batch_size=3)\n",
        "model.save('chatbot-model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "FgFuyLjT_KPe",
        "outputId": "0ee6bf9f-066b-4f1e-b32c-75ef2b08799f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nThis code builds and trains a neural network model using Keras with a TensorFlow\\nbackend for a chatbot or natural language processing task. Let\\'s go through the code step by step:\\n\\n1. `sgd = SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)`: This line creates\\nan instance of the Stochastic Gradient Descent (SGD) optimizer. The optimizer is configured with\\nspecific hyperparameters:\\n   - `learning_rate`: The learning rate controls the step size at each iteration when updating the\\n   model\\'s weights. A small learning rate ensures small updates, which may lead to stable\\n   convergence, but may take longer to train.\\n   - `decay`: Learning rate decay reduces the learning rate over time to help fine-tune the model\\n   towards the end of training. It allows the model to make larger updates in the early stages and\\n   smaller updates later.\\n   - `momentum`: The momentum parameter helps the optimizer to move more consistently in\\n   the relevant direction and dampens oscillations during training. A value of 0.9 is commonly used.\\n   - `nesterov`: Nesterov momentum improves the convergence of the SGD optimizer by using the\\n   \"Nesterov Accelerated Gradient\" variant. It helps the optimizer to make a more informed decision before updating the weights.\\n\\n2. `model = Sequential()`: This line creates a sequential model, which is a linear stack of layers.\\nIn this case, the model consists of multiple layers stacked on top of each other.\\n\\n3. Model Architecture:\\n   - `model.add(Dense(128, input_shape=(len(train_x[0]),), activation=\\'relu\\'))`: The first layer is a\\n   dense layer (fully connected layer) with 128 units (neurons) and the ReLU (Rectified Linear Unit)\\n   activation function. It takes input of shape `(len(train_x[0]),)`, which corresponds to the number\\n   of features in the input data (`train_x`). This is the input layer of the neural network.\\n   - `model.add(Dropout(0.5))`: The dropout layer with a rate of 0.5 is added after the first dense\\n   layer. Dropout is a regularization technique that randomly sets a fraction of input units to zero\\n   during training, which helps prevent overfitting.\\n   - `model.add(Dense(64, activation=\\'relu\\'))`: The second dense layer has 64 units and the ReLU\\n   activation function.\\n   - `model.add(Dropout(0.5))`: Another dropout layer with a rate of 0.5 is added after the second\\n   dense layer.\\n   - `model.add(Dense(len(train_y[0]), activation=\\'softmax\\'))`: The output layer is a dense layer\\n   with the number of units equal to the number of unique classes (intent tags) in the training\\n   data (`train_y`). It uses the softmax activation function to convert the raw output scores into\\n   probabilities. Softmax is suitable for multi-class classification tasks.\\n\\n4. `model.compile(loss=\\'categorical_crossentropy\\', optimizer=sgd, metrics=[\\'accuracy\\'])`: This\\nline compiles the model with the specified configuration. It sets the loss function to\\n`categorical_crossentropy`, which is appropriate for multi-class classification problems. The\\noptimizer is set to the previously defined `sgd` optimizer, and the metric for evaluation during\\ntraining is set to accuracy.\\n\\n5. `model.fit(np.array(train_x), np.array(train_y), epochs=250, batch_size=3)`: This line starts the\\ntraining process. It uses the `fit()` method to train the model on the training data (`train_x`) and\\ntarget labels (`train_y`). The `epochs` parameter determines the number of times the entire\\ndataset is passed through the model during training. In this case, the model is trained for\\n250 epochs. The `batch_size` parameter specifies the number of samples per gradient update.\\nSmaller batch sizes might lead to noisier weight updates, but they can also speed up training.\\n\\n6. `model.save(\\'chatbot-model.h5\\')`: After training, this line saves the trained model to a file\\nnamed `\\'chatbot-model.h5\\'`. The `.h5` extension is commonly used for saving Keras models.\\n\\nThe resulting saved model (`\\'chatbot-model.h5\\'`) can be loaded and used for making predictions\\nin a chatbot or natural language processing application.\\n'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "This code builds and trains a neural network model using Keras with a TensorFlow\n",
        "backend for a chatbot or natural language processing task. Let's go through the code step by step:\n",
        "\n",
        "1. `sgd = SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)`: This line creates\n",
        "an instance of the Stochastic Gradient Descent (SGD) optimizer. The optimizer is configured with\n",
        "specific hyperparameters:\n",
        "   - `learning_rate`: The learning rate controls the step size at each iteration when updating the\n",
        "   model's weights. A small learning rate ensures small updates, which may lead to stable\n",
        "   convergence, but may take longer to train.\n",
        "   - `decay`: Learning rate decay reduces the learning rate over time to help fine-tune the model\n",
        "   towards the end of training. It allows the model to make larger updates in the early stages and\n",
        "   smaller updates later.\n",
        "   - `momentum`: The momentum parameter helps the optimizer to move more consistently in\n",
        "   the relevant direction and dampens oscillations during training. A value of 0.9 is commonly used.\n",
        "   - `nesterov`: Nesterov momentum improves the convergence of the SGD optimizer by using the\n",
        "   \"Nesterov Accelerated Gradient\" variant. It helps the optimizer to make a more informed decision before updating the weights.\n",
        "\n",
        "2. `model = Sequential()`: This line creates a sequential model, which is a linear stack of layers.\n",
        "In this case, the model consists of multiple layers stacked on top of each other.\n",
        "\n",
        "3. Model Architecture:\n",
        "   - `model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))`: The first layer is a\n",
        "   dense layer (fully connected layer) with 128 units (neurons) and the ReLU (Rectified Linear Unit)\n",
        "   activation function. It takes input of shape `(len(train_x[0]),)`, which corresponds to the number\n",
        "   of features in the input data (`train_x`). This is the input layer of the neural network.\n",
        "   - `model.add(Dropout(0.5))`: The dropout layer with a rate of 0.5 is added after the first dense\n",
        "   layer. Dropout is a regularization technique that randomly sets a fraction of input units to zero\n",
        "   during training, which helps prevent overfitting.\n",
        "   - `model.add(Dense(64, activation='relu'))`: The second dense layer has 64 units and the ReLU\n",
        "   activation function.\n",
        "   - `model.add(Dropout(0.5))`: Another dropout layer with a rate of 0.5 is added after the second\n",
        "   dense layer.\n",
        "   - `model.add(Dense(len(train_y[0]), activation='softmax'))`: The output layer is a dense layer\n",
        "   with the number of units equal to the number of unique classes (intent tags) in the training\n",
        "   data (`train_y`). It uses the softmax activation function to convert the raw output scores into\n",
        "   probabilities. Softmax is suitable for multi-class classification tasks.\n",
        "\n",
        "4. `model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])`: This\n",
        "line compiles the model with the specified configuration. It sets the loss function to\n",
        "`categorical_crossentropy`, which is appropriate for multi-class classification problems. The\n",
        "optimizer is set to the previously defined `sgd` optimizer, and the metric for evaluation during\n",
        "training is set to accuracy.\n",
        "\n",
        "5. `model.fit(np.array(train_x), np.array(train_y), epochs=250, batch_size=3)`: This line starts the\n",
        "training process. It uses the `fit()` method to train the model on the training data (`train_x`) and\n",
        "target labels (`train_y`). The `epochs` parameter determines the number of times the entire\n",
        "dataset is passed through the model during training. In this case, the model is trained for\n",
        "250 epochs. The `batch_size` parameter specifies the number of samples per gradient update.\n",
        "Smaller batch sizes might lead to noisier weight updates, but they can also speed up training.\n",
        "\n",
        "6. `model.save('chatbot-model.h5')`: After training, this line saves the trained model to a file\n",
        "named `'chatbot-model.h5'`. The `.h5` extension is commonly used for saving Keras models.\n",
        "\n",
        "The resulting saved model (`'chatbot-model.h5'`) can be loaded and used for making predictions\n",
        "in a chatbot or natural language processing application.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6ViSiow_KSX"
      },
      "outputs": [],
      "source": [
        "# Load the trained model\n",
        "model = load_model('chatbot-model.h5')\n",
        "\n",
        "def clean_up_sentence(sentence):\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    sentence_words = [WordNetLemmatizer().lemmatize(word.lower()) for word in sentence_words]\n",
        "    return sentence_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0qniJLX_KVx"
      },
      "outputs": [],
      "source": [
        "def bow(sentence, words):\n",
        "    bag = [0] * len(words)\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "\n",
        "    for w in sentence_words:\n",
        "        for i, word in enumerate(words):\n",
        "            if word == w:\n",
        "                bag[i] = 1\n",
        "\n",
        "    return np.array(bag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2qKfCex_KYU"
      },
      "outputs": [],
      "source": [
        "def predict_class(sentence, model):\n",
        "    p = bow(sentence, words)\n",
        "    res = model.predict(np.array([p]))[0]\n",
        "    error_threshold = 0.25\n",
        "    results = [[i, r] for i, r in enumerate(res) if r > error_threshold]\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "\n",
        "    for r in results:\n",
        "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
        "    return return_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsCnPiY5_KbU"
      },
      "outputs": [],
      "source": [
        "def get_response(intents_list, intents_json):\n",
        "    tag = intents_list[0]['intent']\n",
        "    list_of_intents = intents_json['intents']\n",
        "\n",
        "    for intent in list_of_intents:\n",
        "        if intent['tag'] == tag:\n",
        "            return random.choice(intent['responses'])\n",
        "\n",
        "    return \"Sorry, I can't understand you.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oN5QVdrZ_KeY"
      },
      "outputs": [],
      "source": [
        "def chatbot_response(text):\n",
        "    intents_list = predict_class(text, model)\n",
        "    res = get_response(intents_list, intents)\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "yba-PHtG_Kh9",
        "outputId": "31049812-d5a3-45c9-deaa-9948351e88c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 22ms/step\n",
            "CodeClause: You will get your offer letter on mail around starting week of the month for which you've applied for the internship\n"
          ]
        }
      ],
      "source": [
        "# Test the chatbot\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    response = chatbot_response(user_input)\n",
        "    print(\"CodeClause:\", response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}